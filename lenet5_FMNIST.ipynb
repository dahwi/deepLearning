{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LWQzjwLWbWJP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlS2cwzX9uAx",
        "outputId": "f063b08d-9dc7-439a-eaf7-62ce51a951c5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgAcIKPZ_sXz"
      },
      "outputs": [],
      "source": [
        "# Mount files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Loading images and labels from .gz files\n",
        "def load_mnist_images(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        # First 16 bytes are metadata about the file\n",
        "        f.read(16)\n",
        "        # The remaining bytes are the image pixels\n",
        "        buf = f.read()\n",
        "        images = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        # Reshape into [number of images, height, width]\n",
        "        images = images.reshape(-1, 28, 28) / 255.0  # normalize to [0, 1] range\n",
        "    return torch.tensor(images).unsqueeze(1)  # add channel dimension (1, 28, 28)\n",
        "\n",
        "def load_mnist_labels(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        # First 8 bytes are metadata about the file\n",
        "        f.read(8)\n",
        "        # The remaining bytes are the label data\n",
        "        buf = f.read()\n",
        "        labels = np.frombuffer(buf, dtype=np.uint8)\n",
        "    return torch.tensor(labels)\n",
        "\n",
        "# Load training data\n",
        "train_images = load_mnist_images('/content/drive/MyDrive/Colab Notebooks/data/train-images-idx3-ubyte.gz')\n",
        "train_labels = load_mnist_labels('/content/drive/MyDrive/Colab Notebooks/data/train-labels-idx1-ubyte.gz')\n",
        "\n",
        "# Load test data\n",
        "test_images = load_mnist_images('/content/drive/MyDrive/Colab Notebooks/data/test-images-idx3-ubyte.gz')\n",
        "test_labels = load_mnist_labels('/content/drive/MyDrive/Colab Notebooks/data/test-labels-idx1-ubyte.gz')\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "train_dataset = TensorDataset(train_images, train_labels)\n",
        "test_dataset = TensorDataset(test_images, test_labels)\n",
        "\n",
        "# Split training dataset into training set and validation set (80, 20)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O2lewKKAGTW"
      },
      "source": [
        "# Setting up models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku0D0Whsjs3N"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Batchnorm after RELU\n",
        "2. One dropout after fc2\n",
        "\"\"\"\n",
        "\n",
        "class LeNet5_BN_After_Relu_One_DropOut(nn.Module):\n",
        "    def __init__(self, dropout=False, batch_norm=False, dropout_p = 0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_p) if dropout else nn.Identity()\n",
        "        self.batch_norm1 = nn.BatchNorm2d(6) if batch_norm else nn.Identity()\n",
        "        self.batch_norm2 = nn.BatchNorm2d(16) if batch_norm else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batch_norm1(x) # no-op if batch_norm is False\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm2(x) # no-op if batch_norm is False\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 16*4*4) # can be replaced with torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x) # no-op if dropout is False\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoEtBPrD28p3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Batchnorm before RELU, instead of after\n",
        "2. Two dropouts: once after fc1 + once after fc2\n",
        "\"\"\"\n",
        "\n",
        "class LeNet5_BN_Before_Relu_Two_DropOut(nn.Module):\n",
        "    def __init__(self, dropout=False, batch_norm=False, dropout_p = 0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_p) if dropout else nn.Identity()\n",
        "        self.batch_norm1 = nn.BatchNorm2d(6) if batch_norm else nn.Identity()\n",
        "        self.batch_norm2 = nn.BatchNorm2d(16) if batch_norm else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer + batchnorm + pooling\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Second layer + batchnorm + pooling\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flatten for FC layers\n",
        "        x = x.view(-1, 16*4*4)\n",
        "\n",
        "        # FC layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfDFOi-TAGk0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BEST\n",
        "1. Batchnorm before RELU, instead of after\n",
        "2. One dropout: once after fc2\n",
        "\"\"\"\n",
        "\n",
        "class LeNet5_BN_Before_Relu_One_DropOut(nn.Module):\n",
        "    def __init__(self, dropout=False, batch_norm=False, dropout_p = 0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_p) if dropout else nn.Identity()\n",
        "        self.batch_norm1 = nn.BatchNorm2d(6) if batch_norm else nn.Identity()\n",
        "        self.batch_norm2 = nn.BatchNorm2d(16) if batch_norm else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer + batchnorm + pooling\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Second layer + batchnorm + pooling\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flatten for FC layers\n",
        "        x = x.view(-1, 16*4*4)\n",
        "\n",
        "        # FC layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5vLP8YOgOGj"
      },
      "outputs": [],
      "source": [
        "def train_model(model, loss_fn, optimizer, epochs=10, measure_without_dropout=False):\n",
        "    best_val_accuracy = 0.0\n",
        "    best_model_state = None\n",
        "    train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            # Move images and labels to the same device as the model\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            # Adjust learning weights\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "\n",
        "        # for model using dropout regularization, go over the training set and measure the accuracy wo dropout\n",
        "        if measure_without_dropout:\n",
        "            train_accuracy = evaluate_model(model, train_loader)\n",
        "            model.train()  # set back to train=true\n",
        "\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        val_accuracy = evaluate_model(model, val_loader)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # test the model for each epoch\n",
        "        test_accuracy = evaluate_model(model, test_loader)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Accuracy: {train_accuracy}%, Validation Accuracy: {val_accuracy}%, Test Accuracy: {test_accuracy}%\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            # best_model_state = model.state_dict() # shallow copy\n",
        "            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}  # deep copy\n",
        "\n",
        "    # Load the best model state\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_accuracies, val_accuracies, test_accuracies\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Disable gradient computation and reduce memory consumption\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            # Move images and labels to the same device as the model\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PVGY6cFAu5w"
      },
      "source": [
        "# Train different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the models and their configurations\n",
        "models_config = {\n",
        "    \"model_no_reg\": [\n",
        "        {\"model\": {\"name\": \"no regularization\", \"module\": LeNet5_BN_Before_Relu_One_DropOut().to(device)}}\n",
        "    ],\n",
        "    \"model_with_dropout\": [\n",
        "        {\"model\": {\"name\": \"two dropouts with 0.3 probability\", \"module\": LeNet5_BN_Before_Relu_Two_DropOut(dropout=True, dropout_p=0.3).to(device)}}, \n",
        "        {\"model\": {\"name\": \"two dropouts with 0.5 probability\", \"module\": LeNet5_BN_Before_Relu_Two_DropOut(dropout=True, dropout_p=0.5).to(device)}},\n",
        "        {\"model\": {\"name\": \"two dropouts with 0.7 probability\", \"module\": LeNet5_BN_Before_Relu_Two_DropOut(dropout=True, dropout_p=0.7).to(device)}},\n",
        "        {\"model\": {\"name\": \"one dropout with 0.3 probability\", \"module\": LeNet5_BN_After_Relu_One_DropOut(dropout=True, dropout_p=0.3).to(device)}},\n",
        "        {\"model\": {\"name\": \"one dropout with 0.5 probability\", \"module\": LeNet5_BN_After_Relu_One_DropOut(dropout=True, dropout_p=0.5).to(device)}}\n",
        "    ],\n",
        "    \"model_with_batchnorm\": [\n",
        "        {\"model\": {\"name\": \"batchnorm before RELU\", \"module\": LeNet5_BN_Before_Relu_One_DropOut(batch_norm=True).to(device)}},\n",
        "        {\"model\": {\"name\": \"batchnorm after RELU\", \"module\": LeNet5_BN_After_Relu_One_DropOut(batch_norm=True).to(device)}}\n",
        "    ],\n",
        "    \"model_with_weight_decay\": [\n",
        "        {\"model\": {\"name\": \"weight decay 1e-2\", \"module\": LeNet5_BN_Before_Relu_One_DropOut().to(device)}, \"params\": {\"weight_decay\": 1e-2}},\n",
        "        {\"model\": {\"name\": \"weight decay 1e-3\", \"module\": LeNet5_BN_Before_Relu_One_DropOut().to(device)}, \"params\": {\"weight_decay\": 1e-3}},\n",
        "        {\"model\": {\"name\": \"weight decay 1e-4\", \"module\": LeNet5_BN_Before_Relu_One_DropOut().to(device)}, \"params\": {\"weight_decay\": 1e-4}}\n",
        "    ]\n",
        "}\n",
        "# Initialize the result dictionary\n",
        "result = {}\n",
        "\n",
        "# Iterate over learning rates and models\n",
        "for model_name, model_info in models_config.items():\n",
        "    for model_dict in model_info:\n",
        "        initial_state = {}  \n",
        "        for learning_rate in [1e-2, 1e-3, 1e-4]:\n",
        "            name = model_dict.get(\"model\", {}).get(\"name\", model_name+\"default\")\n",
        "            model = model_dict.get(\"model\", {}).get(\"module\", LeNet5_BN_Before_Relu_One_DropOut().to(device))\n",
        "            params = model_dict.get(\"params\", {})\n",
        "\n",
        "            # Save the initial state of the model\n",
        "            if learning_rate == 1e-2: \n",
        "               initial_state = {k: v.clone() for k, v in model.state_dict().items()}  # deep copy\n",
        "\n",
        "            # Reset the model's parameters at the beginning of each learning rate iteration\n",
        "            else:\n",
        "              model.load_state_dict(initial_state)\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate, **params)\n",
        "\n",
        "            # Train the model\n",
        "            model_optimized, train_accuracies, val_accuracies, test_accuracies = train_model(model, loss_fn, optimizer)\n",
        "\n",
        "            # Evaluate the model on the test set\n",
        "            test_accuracy = evaluate_model(model_optimized, test_loader)\n",
        "\n",
        "            # Update the result dictionary with the maximum test accuracy\n",
        "            if model_name not in result or test_accuracy > result[model_name]['test_accuracy']:\n",
        "                model_optimized_state = {k: v.clone() for k, v in model_optimized.state_dict().items()}  # deep copy\n",
        "                result[model_name] = {\n",
        "                    'name': name,\n",
        "                    'model': model_optimized,\n",
        "                    'parameters': model_optimized_state,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'test_accuracy': test_accuracy,\n",
        "                    'train_accuracies': train_accuracies,\n",
        "                    'val_accuracies': val_accuracies,\n",
        "                    'test_accuracies': test_accuracies\n",
        "                }\n",
        "            print(f\"Model: {model_name}, desc: {name}, Learning rate: {learning_rate}, Test Accuracy: {test_accuracy}%\")\n",
        "\n",
        "# Print the final results\n",
        "print(\"Final Results:\")\n",
        "for model_name, info in result.items():\n",
        "    print(f\"Model: {model_name}, Best Learning Rate: {info['learning_rate']}, Best Test Accuracy: {info['test_accuracy']}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_rV1n_D0XsW",
        "outputId": "4b3e8e92-e291-44ee-f395-cdc4617f1564"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, 11)\n",
        "\n",
        "def plot_accuracy(epochs, train_accuracies, test_accuracies, title, color):\n",
        "    plt.plot(epochs, train_accuracies, color=color, label='Train Accuracy')\n",
        "    plt.plot(epochs, test_accuracies, color=color, linestyle='--', label='Test Accuracy')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "# Plotting convergence graphs\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_no_reg']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='r')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_with_dropout']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='g')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_with_batchnorm']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='b')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_with_weight_decay']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vKxcChAzx8"
      },
      "source": [
        "# Plotting graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "s1joeW75m_wO",
        "outputId": "cca76f1e-a8c1-471f-d850-bc987b23fc74"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, 11)\n",
        "\n",
        "def plot_accuracy(epochs, train_accuracies, test_accuracies, title, color):\n",
        "    plt.plot(epochs, train_accuracies, color=color, label='Train Accuracy')\n",
        "    plt.plot(epochs, test_accuracies, color=color, linestyle='--', label='Test Accuracy')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "# Plotting convergence graphs\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_no_reg']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='r')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_with_dropout']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='g')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_with_batchnorm']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='b')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.axhline(y=88, color='black', linestyle='-', label='88% Threshold')\n",
        "model = result['model_with_weight_decay']\n",
        "plot_accuracy(epochs, model['train_accuracies'], model['test_accuracies'], model['name'], color='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_to_test = result[\"model_with_dropout\"][\"model\"]\n",
        "model_params = result[\"model_with_dropout\"][\"parameters\"]\n",
        "model_to_test.load_state_dict(model_params)\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = evaluate_model(model_to_test, test_loader)\n",
        "print(test_accuracy)\n",
        "print(test_accuracy == result[\"model_with_dropout\"][\"test_accuracy\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
