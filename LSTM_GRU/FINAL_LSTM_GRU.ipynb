{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyK5h4iU6V9O",
        "outputId": "ef3a8feb-5173-44d8-c29b-c5a0eb9334ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "\n",
        "!wandb login 9172fb113e07d174f618e9042047cc5c4adacc0f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "635B0VpqeaV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56f3363-6006-4fdb-b4dd-45b416841803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "# Mount files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTHSbOEIeibJ",
        "outputId": "6478669d-de34-4ccd-b459-e6c42a556970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "XRf3z5CrCwiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et4vfwgVek2a",
        "outputId": "c9846f8d-b97e-4ecf-9ea3-1a4dc20904f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (929588,)\n",
            "Validation set shape: (73759,)\n",
            "Test set shape: (82429,)\n",
            "Vocabulary size: 10000\n",
            "[ 237  807  950 1325 1476 1691 3773 3920 4067 4380 4731 4922 5569 5732\n",
            " 5876 7091 7175 7366 7769 8203]\n",
            "[8301 8478 8819 9658   43 6605   44   45 9965 6172 9838 4833 9012 1040\n",
            "  609   48 6033 2631 6074   45]\n"
          ]
        }
      ],
      "source": [
        "def data_init():\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/data/ptb.train.txt\") as f:\n",
        "        train = f.read().strip().replace('\\n', '<eos>').split()\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/data/ptb.valid.txt\") as f:\n",
        "        val = f.read().strip().replace('\\n', '<eos>').split()\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/data/ptb.test.txt\") as f:\n",
        "        test = f.read().strip().replace('\\n', '<eos>').split()\n",
        "\n",
        "    words = sorted(set(train))\n",
        "    word2idx = {word: idx for idx, word in enumerate(words)}\n",
        "    trn = [word2idx[w] for w in train]\n",
        "    vld = [word2idx[w] if w in word2idx else word2idx['<unk>'] for w in val]\n",
        "    tst = [word2idx[w] if w in word2idx else word2idx['<unk>'] for w in test]\n",
        "\n",
        "    return np.array(trn), np.array(vld), np.array(tst), len(words)\n",
        "\n",
        "train_set, val_set, test_set, vocab_size = data_init()\n",
        "\n",
        "print(\"Train set shape:\", train_set.shape)\n",
        "print(\"Validation set shape:\", val_set.shape)\n",
        "print(\"Test set shape:\", test_set.shape)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(train_set[:20])\n",
        "print(train_set[20:40])\n",
        "\n",
        "# print(len(train_text), train_text[:10])\n",
        "# print(len(valid_text), valid_text[:10])\n",
        "# print(len(test_text), test_text[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch data preparation\n",
        "def minibatch(data, batch_size, seq_length):\n",
        "    data = torch.tensor(data, dtype=torch.int64)\n",
        "    num_batches = data.size(0) // batch_size\n",
        "    data = data[:num_batches * batch_size].view(batch_size, -1)\n",
        "\n",
        "    dataset = []\n",
        "    for i in range(0, data.size(1) - seq_length+1, seq_length):\n",
        "        x = data[:, i:i + seq_length].transpose(1, 0)\n",
        "        y = data[:, i+1:i+seq_length+1].transpose(1, 0)\n",
        "        dataset.append((x, y))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "VkdQgSpE3B6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing minibatch\n",
        "batch_size = 20\n",
        "seq_length = 20\n",
        "\n",
        "train_batch = minibatch(train_set, batch_size, seq_length)\n",
        "valid_batch = minibatch(val_set, batch_size, seq_length)\n",
        "test_batch = minibatch(test_set, batch_size, seq_length)\n",
        "\n",
        "print(len(train_batch))\n",
        "print(len(valid_batch))\n",
        "print(len(test_batch))\n",
        "\n",
        "print(train_batch[0][0].shape)\n",
        "print(train_batch[0][1].shape)\n",
        "print(train_batch[0][0])\n",
        "print(train_batch[0][1])\n",
        "print(\"*********\")\n",
        "\n",
        "# for i, (x, y) in enumerate(valid_batch):\n",
        "#     print(f\"Batch {i}: x shape: {x.shape}, y shape: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtSkAiVx3oYL",
        "outputId": "dc360bba-ea77-4c67-cdf2-a6907aefd67e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2323\n",
            "184\n",
            "206\n",
            "torch.Size([20, 20])\n",
            "torch.Size([20, 20])\n",
            "tensor([[ 237, 9010,   45,  424,  657, 5133,   43, 9846, 4769, 8215, 6237, 2863,\n",
            "          873, 7829,  424,   45, 7877,  406, 5442,   95],\n",
            "        [ 807, 9928, 8093, 9805, 5086, 9012, 1097, 9012, 8713, 9961, 5234,   43,\n",
            "         5782,   43,   44,   45, 5825, 6142, 1965, 6237],\n",
            "        [ 950, 8304, 6142, 6378, 4470, 2764, 9888,   44,  609,   43, 9119, 1573,\n",
            "         2602, 9119, 9869,  424,   44, 2362, 5799, 3352],\n",
            "        [1325, 5232, 4770,   43, 6400, 8307, 7875, 9012, 9590, 3756,   48, 2096,\n",
            "           43, 8860,  345,  270, 9012,  812, 6142, 2343],\n",
            "        [1476,   48, 1800, 9010,  889, 9055, 9034, 3908, 7298, 1551, 4039,  413,\n",
            "         5825, 9012, 9010, 6185, 2947,  555, 4619,    3],\n",
            "        [1691, 1406, 8582, 5531, 6142, 5755,  373, 9846,  424, 4028, 9012,   44,\n",
            "         7728, 6919, 5531,  536, 6142, 9842, 1717, 6920],\n",
            "        [3773, 9474, 3659, 3957, 1283,  280, 3543, 9012, 6532, 6257, 3972, 1315,\n",
            "         1768, 5993, 3659,   45,   44, 9119, 9705, 4470],\n",
            "        [3920, 9869, 2897, 2124, 6965,  373, 4764,   44, 1868, 7806, 1342,  994,\n",
            "          148, 6326, 7322,   45,   43, 3800, 3179, 9012],\n",
            "        [4067, 2362, 6142,  873, 3876, 4017,   44,   43,  424, 7824, 3098, 3563,\n",
            "          609, 4155, 5098,   43, 9869,  424,  657, 9668],\n",
            "        [4380, 9010,   44, 5417, 1291, 8304, 9010, 5825, 9024, 9751, 9012, 4748,\n",
            "           48, 7008, 2121, 4764, 6063, 9012,   73, 6142],\n",
            "        [4731, 4764, 2498, 1678, 8583, 7198, 1452, 3079, 8002, 4764, 7843, 7977,\n",
            "           44, 8056,  424, 9705, 9034, 6870,   45, 9012],\n",
            "        [4922, 9928,    9, 9119,  543,   44, 5985,  424, 9020, 4144, 3628, 4403,\n",
            "         3263,   48,  963, 2884,    8, 1180,  987,   44],\n",
            "        [5569,   44,   45, 2265,   48, 4758, 4748, 5825,  657, 5347, 6142, 9012,\n",
            "         5825,   44, 7064, 9054,   44,  311, 9889, 6167],\n",
            "        [5732, 7820, 8093, 7871, 3681,   44,  493, 9051,   48,   48, 7544, 6573,\n",
            "         6573, 9661, 7407, 4181, 2713, 9928,    1, 8435],\n",
            "        [5876, 6237, 6296, 8132, 6142, 9696, 1878, 2843, 4353,  971,  424, 9966,\n",
            "         4144, 7878,   43, 9010,   43,  873,   45,   43],\n",
            "        [7091,   48,   43, 2121, 6961, 6142, 6185, 4470,   44, 3659, 7321, 1342,\n",
            "           48, 9119, 4764, 9012, 2350, 9842, 5667, 5177],\n",
            "        [7175, 1406, 5156, 4165, 9195, 4555, 5825, 2602,   43, 2041, 9012,  873,\n",
            "           44, 9304, 4212, 5900, 9881, 9119, 5027, 9059],\n",
            "        [7366,    9, 5901, 7824, 4729,   43, 4279,   44, 9012, 6142, 2254, 9504,\n",
            "           44, 9119, 9119, 3568, 7646, 6448, 9958, 6142],\n",
            "        [7769,   63,   48,   43, 3819, 1279,    9, 4470, 3946, 1707, 2423, 9119,\n",
            "           43, 7389, 4902, 5079, 4477,   43,   43, 7833],\n",
            "        [8203, 9119,  800, 5783, 2031, 9048, 6920, 9012, 7202, 1204,   43, 5378,\n",
            "         2884, 4470, 6275, 9012, 5286, 5825, 1135,  900]])\n",
            "tensor([[ 807, 9928, 8093, 9805, 5086, 9012, 1097, 9012, 8713, 9961, 5234,   43,\n",
            "         5782,   43,   44,   45, 5825, 6142, 1965, 6237],\n",
            "        [ 950, 8304, 6142, 6378, 4470, 2764, 9888,   44,  609,   43, 9119, 1573,\n",
            "         2602, 9119, 9869,  424,   44, 2362, 5799, 3352],\n",
            "        [1325, 5232, 4770,   43, 6400, 8307, 7875, 9012, 9590, 3756,   48, 2096,\n",
            "           43, 8860,  345,  270, 9012,  812, 6142, 2343],\n",
            "        [1476,   48, 1800, 9010,  889, 9055, 9034, 3908, 7298, 1551, 4039,  413,\n",
            "         5825, 9012, 9010, 6185, 2947,  555, 4619,    3],\n",
            "        [1691, 1406, 8582, 5531, 6142, 5755,  373, 9846,  424, 4028, 9012,   44,\n",
            "         7728, 6919, 5531,  536, 6142, 9842, 1717, 6920],\n",
            "        [3773, 9474, 3659, 3957, 1283,  280, 3543, 9012, 6532, 6257, 3972, 1315,\n",
            "         1768, 5993, 3659,   45,   44, 9119, 9705, 4470],\n",
            "        [3920, 9869, 2897, 2124, 6965,  373, 4764,   44, 1868, 7806, 1342,  994,\n",
            "          148, 6326, 7322,   45,   43, 3800, 3179, 9012],\n",
            "        [4067, 2362, 6142,  873, 3876, 4017,   44,   43,  424, 7824, 3098, 3563,\n",
            "          609, 4155, 5098,   43, 9869,  424,  657, 9668],\n",
            "        [4380, 9010,   44, 5417, 1291, 8304, 9010, 5825, 9024, 9751, 9012, 4748,\n",
            "           48, 7008, 2121, 4764, 6063, 9012,   73, 6142],\n",
            "        [4731, 4764, 2498, 1678, 8583, 7198, 1452, 3079, 8002, 4764, 7843, 7977,\n",
            "           44, 8056,  424, 9705, 9034, 6870,   45, 9012],\n",
            "        [4922, 9928,    9, 9119,  543,   44, 5985,  424, 9020, 4144, 3628, 4403,\n",
            "         3263,   48,  963, 2884,    8, 1180,  987,   44],\n",
            "        [5569,   44,   45, 2265,   48, 4758, 4748, 5825,  657, 5347, 6142, 9012,\n",
            "         5825,   44, 7064, 9054,   44,  311, 9889, 6167],\n",
            "        [5732, 7820, 8093, 7871, 3681,   44,  493, 9051,   48,   48, 7544, 6573,\n",
            "         6573, 9661, 7407, 4181, 2713, 9928,    1, 8435],\n",
            "        [5876, 6237, 6296, 8132, 6142, 9696, 1878, 2843, 4353,  971,  424, 9966,\n",
            "         4144, 7878,   43, 9010,   43,  873,   45,   43],\n",
            "        [7091,   48,   43, 2121, 6961, 6142, 6185, 4470,   44, 3659, 7321, 1342,\n",
            "           48, 9119, 4764, 9012, 2350, 9842, 5667, 5177],\n",
            "        [7175, 1406, 5156, 4165, 9195, 4555, 5825, 2602,   43, 2041, 9012,  873,\n",
            "           44, 9304, 4212, 5900, 9881, 9119, 5027, 9059],\n",
            "        [7366,    9, 5901, 7824, 4729,   43, 4279,   44, 9012, 6142, 2254, 9504,\n",
            "           44, 9119, 9119, 3568, 7646, 6448, 9958, 6142],\n",
            "        [7769,   63,   48,   43, 3819, 1279,    9, 4470, 3946, 1707, 2423, 9119,\n",
            "           43, 7389, 4902, 5079, 4477,   43,   43, 7833],\n",
            "        [8203, 9119,  800, 5783, 2031, 9048, 6920, 9012, 7202, 1204,   43, 5378,\n",
            "         2884, 4470, 6275, 9012, 5286, 5825, 1135,  900],\n",
            "        [8301, 1829, 4283, 6943,   43,   44,  609, 8687, 3659, 6667, 6803, 4770,\n",
            "         9100, 7827, 9426,   44, 4770, 8572, 5347, 9020]])\n",
            "*********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo05XGyPFW1j"
      },
      "source": [
        "# Defining our models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, dropout, rnn_type='LSTM'):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        # Embedding layer to map input tokens to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        # RNN layer (either LSTM or GRU based on user choice)\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, dropout=dropout)\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Linear layer to map from hidden state to vocabulary size (for logits)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    # Initialize parameters to U(-0.1, 0.1)\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            nn.init.uniform_(param, -0.1, 0.1)\n",
        "\n",
        "    # Forward pass: directly from paper\n",
        "    def forward(self, x, states):\n",
        "        x = self.dropout(self.embedding(x))  # Embedding input, then dropout\n",
        "        x, states = self.rnn(x, states)  # Pass through RNN (LSTM or GRU)\n",
        "        x = self.dropout(x)  # Apply dropout after rnn again\n",
        "        x = self.fc(x)  # Final fully connected layer to get logits\n",
        "        return x, states\n",
        "\n",
        "    # Initialize hidden (and cell) states\n",
        "    def state_init(self, batch_size):\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # h0 = torch.randn(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            # c0 = torch.randn(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "            # h0 = torch.nn.init.xavier_uniform_(torch.empty(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
        "            # c0 = torch.nn.init.xavier_uniform_(torch.empty(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
        "\n",
        "            return (h0, c0)\n",
        "        else:  # GRU has only hidden states\n",
        "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            # h0 = torch.randn(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "            # h0 = torch.nn.init.xavier_uniform_(torch.empty(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
        "\n",
        "            return h0\n",
        "\n",
        "    # Detach hidden states (to avoid backpropagating through entire sequence)\n",
        "    def detach(self, states):\n",
        "        if isinstance(states, tuple):  # LSTM states\n",
        "            return (states[0].detach(), states[1].detach())\n",
        "        else:  # GRU state\n",
        "            return states.detach()\n"
      ],
      "metadata": {
        "id": "7OoDrJTRLYlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity calculation\n",
        "def perplexity(data, model, batch_size):\n",
        "    with torch.no_grad():\n",
        "        losses = []\n",
        "        states = model.state_init(batch_size)\n",
        "        for x, y in data:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            scores, states = model(x, states)\n",
        "            loss = cross_entropy_loss(scores, y)\n",
        "            losses.append(loss.item())\n",
        "    return np.exp(np.mean(losses))"
      ],
      "metadata": {
        "id": "0cZgojuXF0mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(scores, y):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scores = scores.reshape(-1, scores.size(2))\n",
        "    y = y.reshape(-1)\n",
        "    loss = criterion(scores, y)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "uts8wucZIiN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "def train(data, model, epochs, initial_learning_rate, id, max_grad_norm, epoch_threshold, lr_decay, step_size=6, gamma=1.0/1.65, dropout=False):\n",
        "    wandb.init(\n",
        "        project=\"dl-ex2\",\n",
        "        name=f'{model.rnn_type}_lr_{initial_learning_rate}_dropout_{model.dropout.p}',\n",
        "        config={\n",
        "        \"learning_rate\": initial_learning_rate,\n",
        "        \"architecture\": model.rnn_type,\n",
        "        \"hidden_size\": model.hidden_size,\n",
        "        \"layer_num\": model.num_layers,\n",
        "        \"epochs\": epochs,\n",
        "        \"dropout\": model.dropout.p,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"seq_length\": seq_length,\n",
        "        \"max_grad_norm\": max_grad_norm\n",
        "        }\n",
        "    )\n",
        "\n",
        "    trn, vld, tst = data\n",
        "    tic = timeit.default_timer()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate)\n",
        "    if dropout:\n",
        "      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decay learning rate every (step_size) epochs by a factor of 0.5\n",
        "    else:\n",
        "      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decay learning rate every (step_size) epochs by a factor of 0.5\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        states = model.state_init(batch_size)\n",
        "        total_loss = 0.0\n",
        "        total_words = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(trn):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            states = model.detach(states)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            scores, states = model(x, states)\n",
        "\n",
        "            # Loss and Backpropagation\n",
        "            loss = cross_entropy_loss(scores, y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_words += y.numel()\n",
        "            # Print 10 times per batch\n",
        "            if i % (len(trn)//10) == 0:\n",
        "                toc = timeit.default_timer()\n",
        "                print(\"batch no = {:d} / {:d}, \".format(i, len(trn)) +\n",
        "                      # \"avg train loss per word this batch = {:.3f}, \".format(loss.item()/(y.numel())) +\n",
        "                      \"avg train loss per word this batch = {:.3f}, \".format(loss.item()) +\n",
        "                      \"words per second = {:d}, \".format(round(total_words/(toc-tic))) +\n",
        "                      \"lr = {:.3f}, \".format(optimizer.param_groups[0]['lr']) +\n",
        "                      \"since beginning = {:d} mins, \".format(round((toc-tic)/60)))\n",
        "\n",
        "        avg_train_loss = total_loss / len(trn)\n",
        "        train_perp = perplexity(trn, model, batch_size)\n",
        "\n",
        "        # Validation and Test perplexity\n",
        "        model.eval()\n",
        "        val_perp = perplexity(vld, model, batch_size)\n",
        "        test_perp = perplexity(tst, model, batch_size)\n",
        "        print(f\"Epoch {epoch + 1}: Start Learning Rate: {initial_learning_rate}, Dropout: {model.dropout.p}\")\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss:.3f}\")\n",
        "        print(f\"Epoch {epoch + 1}: Train Perplexity: {train_perp:.3f}\")\n",
        "        print(f\"Epoch {epoch + 1}: Validation Perplexity: {val_perp:.3f}\")\n",
        "        print(f\"Epoch {epoch + 1}: Test Perplexity: {test_perp:.3f}\")\n",
        "\n",
        "        # Wandb Plotting\n",
        "        wandb.log({\"Train Perplexity\": train_perp, \"Validation Perplexity\": val_perp, \"Test Perplexity\": test_perp, \"epoch\": epoch, \"learning_rate\": optimizer.param_groups[0]['lr'],\"dropout\": model.dropout.p })\n",
        "\n",
        "        # 1. Use scheduler to decay LR every # of steps\n",
        "        scheduler.step()\n",
        "\n",
        "        # 2. Paper -> Decay LR every step after a certain threshold\n",
        "        # if epoch >= epoch_threshold:\n",
        "        #     for param_group in optimizer.param_groups:\n",
        "        #         param_group['lr'] *= lr_decay  # Decay learning rate\n",
        "\n",
        "        # Save the best model\n",
        "        if val_perp < best_val_loss:\n",
        "            print(f\"Saw better model at Epoch {epoch+1}\")\n",
        "            best_val_loss = val_perp\n",
        "            best_model = {k: v.clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    # Test set perplexity\n",
        "    model.load_state_dict(best_model)\n",
        "    test_perp = perplexity(tst, model, batch_size)\n",
        "    print(f\"Test Set Perplexity: {test_perp:.3f} Model: {model.rnn_type} Dropout: {model.dropout.p} Hidden_size: {model.hidden_size}\")\n",
        "\n",
        "    torch.save(best_model, 'best_model.pth')\n",
        "    print(\"Training complete. Best model saved.\")\n"
      ],
      "metadata": {
        "id": "ee_U0ygyCjA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3WxWyHKFaH7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a5f43e7bd7914a559f06c069630e8e1d",
            "0f25d88e25c341bbae6b550e403a4aca",
            "5638374ee6694af8b9e07ba2eb04613c",
            "9a35e839a51a4dbc92c5623dd3b372a2",
            "9148e9f659b44c65b2ee98bbc94bcc64",
            "64afe86788344687a64dbed17c2bcc7b",
            "5cf89f6e736443d593c25faa73389ee2",
            "49e4ba3f8770464c84735cbbdfddc90b"
          ]
        },
        "id": "zraSpyK8g1su",
        "outputId": "220b412d-28f5-41bb-d22d-f07e0b4b4a76"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2uyt3rlu) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.036 MB of 0.036 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5f43e7bd7914a559f06c069630e8e1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Perplexity</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Train Perplexity</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation Perplexity</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>dropout</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▄▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Perplexity</td><td>114.23718</td></tr><tr><td>Train Perplexity</td><td>89.63245</td></tr><tr><td>Validation Perplexity</td><td>118.69247</td></tr><tr><td>dropout</td><td>0.25</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>learning_rate</td><td>0.69204</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GRU_lr_2.0_dropout_0.25</strong> at: <a href='https://wandb.ai/cornell-tech-dl/dl-ex2/runs/2uyt3rlu' target=\"_blank\">https://wandb.ai/cornell-tech-dl/dl-ex2/runs/2uyt3rlu</a><br/> View project at: <a href='https://wandb.ai/cornell-tech-dl/dl-ex2' target=\"_blank\">https://wandb.ai/cornell-tech-dl/dl-ex2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240923_202257-2uyt3rlu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2uyt3rlu). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240923_202657-r3rse4sx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cornell-tech-dl/dl-ex2/runs/r3rse4sx' target=\"_blank\">GRU_lr_2.0_dropout_0.25</a></strong> to <a href='https://wandb.ai/cornell-tech-dl/dl-ex2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cornell-tech-dl/dl-ex2' target=\"_blank\">https://wandb.ai/cornell-tech-dl/dl-ex2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cornell-tech-dl/dl-ex2/runs/r3rse4sx' target=\"_blank\">https://wandb.ai/cornell-tech-dl/dl-ex2/runs/r3rse4sx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch no = 0 / 2323, avg train loss per word this batch = 9.216, words per second = 48690, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 6.616, words per second = 69077, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 6.297, words per second = 72381, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 6.206, words per second = 73537, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 5.771, words per second = 76391, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 5.819, words per second = 79039, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 6.037, words per second = 80827, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 5.704, words per second = 82181, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 5.485, words per second = 83233, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 5.962, words per second = 84046, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 5.758, words per second = 84718, lr = 2.000, since beginning = 0 mins, \n",
            "Epoch 1: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 1: Train Loss: 6.199\n",
            "Epoch 1: Train Perplexity: 282.439\n",
            "Epoch 1: Validation Perplexity: 266.206\n",
            "Epoch 1: Test Perplexity: 260.349\n",
            "Saw better model at Epoch 1\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 6.143, words per second = 26, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 5.594, words per second = 5626, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 5.550, words per second = 10472, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 5.537, words per second = 14835, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 5.297, words per second = 18748, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 5.448, words per second = 22275, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 5.652, words per second = 25475, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 5.314, words per second = 28381, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 5.081, words per second = 31040, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 5.599, words per second = 33470, lr = 2.000, since beginning = 0 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 5.462, words per second = 35718, lr = 2.000, since beginning = 0 mins, \n",
            "Epoch 2: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 2: Train Loss: 5.468\n",
            "Epoch 2: Train Perplexity: 205.137\n",
            "Epoch 2: Validation Perplexity: 200.719\n",
            "Epoch 2: Test Perplexity: 195.767\n",
            "Saw better model at Epoch 2\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.848, words per second = 13, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 5.252, words per second = 2927, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 5.277, words per second = 5657, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 5.261, words per second = 8224, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 5.018, words per second = 10640, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 5.200, words per second = 12919, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 5.376, words per second = 15071, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 5.207, words per second = 17109, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.835, words per second = 19032, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 5.414, words per second = 20861, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 5.361, words per second = 22567, lr = 2.000, since beginning = 1 mins, \n",
            "Epoch 3: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 3: Train Loss: 5.222\n",
            "Epoch 3: Train Perplexity: 168.024\n",
            "Epoch 3: Validation Perplexity: 170.682\n",
            "Epoch 3: Test Perplexity: 166.370\n",
            "Saw better model at Epoch 3\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.667, words per second = 9, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 5.038, words per second = 1987, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 5.021, words per second = 3882, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 5.094, words per second = 5697, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.910, words per second = 7438, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 5.079, words per second = 9110, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 5.200, words per second = 10716, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 5.017, words per second = 12259, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.722, words per second = 13744, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 5.287, words per second = 15125, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 5.220, words per second = 16452, lr = 2.000, since beginning = 1 mins, \n",
            "Epoch 4: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 4: Train Loss: 5.057\n",
            "Epoch 4: Train Perplexity: 146.343\n",
            "Epoch 4: Validation Perplexity: 153.526\n",
            "Epoch 4: Test Perplexity: 150.061\n",
            "Saw better model at Epoch 4\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.560, words per second = 7, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.852, words per second = 1499, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.933, words per second = 2943, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.920, words per second = 4342, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.780, words per second = 5697, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.977, words per second = 7011, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 5.105, words per second = 8276, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.913, words per second = 9458, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.565, words per second = 10607, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 5.182, words per second = 11710, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 5.036, words per second = 12766, lr = 2.000, since beginning = 1 mins, \n",
            "Epoch 5: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 5: Train Loss: 4.932\n",
            "Epoch 5: Train Perplexity: 130.079\n",
            "Epoch 5: Validation Perplexity: 142.380\n",
            "Epoch 5: Test Perplexity: 139.078\n",
            "Saw better model at Epoch 5\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.510, words per second = 5, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.745, words per second = 1189, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.783, words per second = 2342, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.810, words per second = 3466, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.744, words per second = 4562, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.889, words per second = 5631, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 5.018, words per second = 6670, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.911, words per second = 7671, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.520, words per second = 8645, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 5.062, words per second = 9596, lr = 2.000, since beginning = 1 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.944, words per second = 10539, lr = 2.000, since beginning = 1 mins, \n",
            "Epoch 6: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 6: Train Loss: 4.831\n",
            "Epoch 6: Train Perplexity: 119.288\n",
            "Epoch 6: Validation Perplexity: 135.199\n",
            "Epoch 6: Test Perplexity: 131.841\n",
            "Saw better model at Epoch 6\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.443, words per second = 4, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.668, words per second = 999, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.732, words per second = 1972, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.710, words per second = 2924, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.580, words per second = 3856, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.736, words per second = 4762, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.859, words per second = 5644, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.679, words per second = 6502, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.334, words per second = 7354, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.872, words per second = 8190, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.805, words per second = 9009, lr = 1.176, since beginning = 2 mins, \n",
            "Epoch 7: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 7: Train Loss: 4.701\n",
            "Epoch 7: Train Perplexity: 106.465\n",
            "Epoch 7: Validation Perplexity: 125.494\n",
            "Epoch 7: Test Perplexity: 122.220\n",
            "Saw better model at Epoch 7\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.373, words per second = 4, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.595, words per second = 861, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.738, words per second = 1702, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.641, words per second = 2527, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.523, words per second = 3332, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.717, words per second = 4120, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.839, words per second = 4893, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.655, words per second = 5657, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.267, words per second = 6408, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.840, words per second = 7146, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.744, words per second = 7871, lr = 1.176, since beginning = 2 mins, \n",
            "Epoch 8: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 8: Train Loss: 4.645\n",
            "Epoch 8: Train Perplexity: 101.361\n",
            "Epoch 8: Validation Perplexity: 122.914\n",
            "Epoch 8: Test Perplexity: 119.939\n",
            "Saw better model at Epoch 8\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.328, words per second = 3, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.534, words per second = 757, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.605, words per second = 1496, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.551, words per second = 2221, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.478, words per second = 2931, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.664, words per second = 3633, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.855, words per second = 4325, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.622, words per second = 5005, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.311, words per second = 5675, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.812, words per second = 6335, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.811, words per second = 6984, lr = 1.176, since beginning = 2 mins, \n",
            "Epoch 9: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 9: Train Loss: 4.600\n",
            "Epoch 9: Train Perplexity: 97.540\n",
            "Epoch 9: Validation Perplexity: 121.177\n",
            "Epoch 9: Test Perplexity: 117.951\n",
            "Saw better model at Epoch 9\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.245, words per second = 3, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.495, words per second = 673, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.596, words per second = 1333, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.621, words per second = 1981, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.416, words per second = 2621, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.617, words per second = 3252, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.814, words per second = 3875, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.561, words per second = 4488, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.293, words per second = 5093, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.740, words per second = 5689, lr = 1.176, since beginning = 2 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.696, words per second = 6277, lr = 1.176, since beginning = 2 mins, \n",
            "Epoch 10: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 10: Train Loss: 4.562\n",
            "Epoch 10: Train Perplexity: 93.428\n",
            "Epoch 10: Validation Perplexity: 119.486\n",
            "Epoch 10: Test Perplexity: 116.109\n",
            "Saw better model at Epoch 10\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.235, words per second = 3, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.402, words per second = 606, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.597, words per second = 1202, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.536, words per second = 1790, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.446, words per second = 2370, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.565, words per second = 2943, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.777, words per second = 3508, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.586, words per second = 4066, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.172, words per second = 4617, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.674, words per second = 5161, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.625, words per second = 5699, lr = 1.176, since beginning = 3 mins, \n",
            "Epoch 11: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 11: Train Loss: 4.525\n",
            "Epoch 11: Train Perplexity: 90.857\n",
            "Epoch 11: Validation Perplexity: 118.207\n",
            "Epoch 11: Test Perplexity: 114.871\n",
            "Saw better model at Epoch 11\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.179, words per second = 2, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.345, words per second = 552, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.477, words per second = 1096, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.461, words per second = 1632, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.364, words per second = 2163, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.615, words per second = 2687, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.698, words per second = 3205, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.513, words per second = 3717, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.187, words per second = 4223, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.662, words per second = 4723, lr = 1.176, since beginning = 3 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.591, words per second = 5214, lr = 1.176, since beginning = 3 mins, \n",
            "Epoch 12: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 12: Train Loss: 4.491\n",
            "Epoch 12: Train Perplexity: 88.303\n",
            "Epoch 12: Validation Perplexity: 117.424\n",
            "Epoch 12: Test Perplexity: 114.099\n",
            "Saw better model at Epoch 12\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.165, words per second = 2, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.355, words per second = 507, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.491, words per second = 1007, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.321, words per second = 1501, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.280, words per second = 1989, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.509, words per second = 2473, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.647, words per second = 2951, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.496, words per second = 3424, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.092, words per second = 3890, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.514, words per second = 4349, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.534, words per second = 4802, lr = 0.692, since beginning = 3 mins, \n",
            "Epoch 13: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 13: Train Loss: 4.426\n",
            "Epoch 13: Train Perplexity: 82.863\n",
            "Epoch 13: Validation Perplexity: 113.637\n",
            "Epoch 13: Test Perplexity: 110.039\n",
            "Saw better model at Epoch 13\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.097, words per second = 2, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.363, words per second = 469, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.404, words per second = 932, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.344, words per second = 1389, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.254, words per second = 1842, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.492, words per second = 2291, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.571, words per second = 2735, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.435, words per second = 3172, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.083, words per second = 3604, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.494, words per second = 4030, lr = 0.692, since beginning = 3 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.440, words per second = 4456, lr = 0.692, since beginning = 3 mins, \n",
            "Epoch 14: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 14: Train Loss: 4.398\n",
            "Epoch 14: Train Perplexity: 80.898\n",
            "Epoch 14: Validation Perplexity: 112.886\n",
            "Epoch 14: Test Perplexity: 109.152\n",
            "Saw better model at Epoch 14\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.062, words per second = 2, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.235, words per second = 436, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.408, words per second = 867, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.306, words per second = 1293, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.276, words per second = 1715, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.455, words per second = 2133, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.556, words per second = 2545, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.422, words per second = 2952, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.133, words per second = 3357, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.561, words per second = 3759, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.492, words per second = 4157, lr = 0.692, since beginning = 4 mins, \n",
            "Epoch 15: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 15: Train Loss: 4.380\n",
            "Epoch 15: Train Perplexity: 79.225\n",
            "Epoch 15: Validation Perplexity: 112.083\n",
            "Epoch 15: Test Perplexity: 108.128\n",
            "Saw better model at Epoch 15\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.043, words per second = 2, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.308, words per second = 408, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.432, words per second = 810, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.385, words per second = 1209, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.207, words per second = 1603, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.461, words per second = 1993, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.482, words per second = 2379, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.404, words per second = 2764, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.062, words per second = 3145, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.439, words per second = 3522, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.451, words per second = 3897, lr = 0.692, since beginning = 4 mins, \n",
            "Epoch 16: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 16: Train Loss: 4.361\n",
            "Epoch 16: Train Perplexity: 78.275\n",
            "Epoch 16: Validation Perplexity: 112.302\n",
            "Epoch 16: Test Perplexity: 108.602\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 5.080, words per second = 2, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.261, words per second = 383, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.407, words per second = 761, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.268, words per second = 1134, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.207, words per second = 1505, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.471, words per second = 1872, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.487, words per second = 2237, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.373, words per second = 2599, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.019, words per second = 2958, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.433, words per second = 3314, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.455, words per second = 3667, lr = 0.692, since beginning = 4 mins, \n",
            "Epoch 17: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 17: Train Loss: 4.343\n",
            "Epoch 17: Train Perplexity: 76.996\n",
            "Epoch 17: Validation Perplexity: 111.626\n",
            "Epoch 17: Test Perplexity: 108.129\n",
            "Saw better model at Epoch 17\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.976, words per second = 2, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.238, words per second = 360, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.424, words per second = 716, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.266, words per second = 1068, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.174, words per second = 1419, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.472, words per second = 1766, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.488, words per second = 2111, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.330, words per second = 2453, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.052, words per second = 2792, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.534, words per second = 3129, lr = 0.692, since beginning = 4 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.419, words per second = 3463, lr = 0.692, since beginning = 4 mins, \n",
            "Epoch 18: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 18: Train Loss: 4.328\n",
            "Epoch 18: Train Perplexity: 75.653\n",
            "Epoch 18: Validation Perplexity: 111.645\n",
            "Epoch 18: Test Perplexity: 107.778\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.964, words per second = 1, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.177, words per second = 340, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.336, words per second = 677, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.262, words per second = 1010, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.205, words per second = 1342, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.404, words per second = 1671, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.482, words per second = 1997, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.349, words per second = 2321, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.016, words per second = 2643, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.405, words per second = 2963, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.326, words per second = 3280, lr = 0.407, since beginning = 5 mins, \n",
            "Epoch 19: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 19: Train Loss: 4.289\n",
            "Epoch 19: Train Perplexity: 73.075\n",
            "Epoch 19: Validation Perplexity: 109.344\n",
            "Epoch 19: Test Perplexity: 105.514\n",
            "Saw better model at Epoch 19\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.957, words per second = 1, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.222, words per second = 323, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.299, words per second = 642, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.220, words per second = 958, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.154, words per second = 1273, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.424, words per second = 1585, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.439, words per second = 1895, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.266, words per second = 2203, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.049, words per second = 2509, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.435, words per second = 2813, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.367, words per second = 3114, lr = 0.407, since beginning = 5 mins, \n",
            "Epoch 20: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 20: Train Loss: 4.273\n",
            "Epoch 20: Train Perplexity: 72.032\n",
            "Epoch 20: Validation Perplexity: 109.072\n",
            "Epoch 20: Test Perplexity: 105.346\n",
            "Saw better model at Epoch 20\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.952, words per second = 1, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.125, words per second = 307, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.353, words per second = 610, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.236, words per second = 911, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.107, words per second = 1211, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.344, words per second = 1508, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.446, words per second = 1803, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.362, words per second = 2097, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.970, words per second = 2388, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.436, words per second = 2676, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.328, words per second = 2961, lr = 0.407, since beginning = 5 mins, \n",
            "Epoch 21: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 21: Train Loss: 4.263\n",
            "Epoch 21: Train Perplexity: 71.074\n",
            "Epoch 21: Validation Perplexity: 108.889\n",
            "Epoch 21: Test Perplexity: 104.935\n",
            "Saw better model at Epoch 21\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.968, words per second = 1, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.151, words per second = 292, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.283, words per second = 581, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.205, words per second = 869, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.103, words per second = 1154, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.374, words per second = 1438, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.429, words per second = 1720, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.325, words per second = 1999, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.951, words per second = 2276, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.320, words per second = 2551, lr = 0.407, since beginning = 5 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.270, words per second = 2825, lr = 0.407, since beginning = 5 mins, \n",
            "Epoch 22: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 22: Train Loss: 4.252\n",
            "Epoch 22: Train Perplexity: 70.537\n",
            "Epoch 22: Validation Perplexity: 108.768\n",
            "Epoch 22: Test Perplexity: 104.901\n",
            "Saw better model at Epoch 22\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.959, words per second = 1, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.093, words per second = 279, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.256, words per second = 556, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.241, words per second = 830, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.090, words per second = 1103, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.330, words per second = 1374, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.384, words per second = 1643, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.322, words per second = 1910, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.951, words per second = 2175, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.351, words per second = 2440, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.424, words per second = 2703, lr = 0.407, since beginning = 6 mins, \n",
            "Epoch 23: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 23: Train Loss: 4.245\n",
            "Epoch 23: Train Perplexity: 69.961\n",
            "Epoch 23: Validation Perplexity: 108.428\n",
            "Epoch 23: Test Perplexity: 104.620\n",
            "Saw better model at Epoch 23\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.927, words per second = 1, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.059, words per second = 267, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.349, words per second = 532, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.236, words per second = 795, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.143, words per second = 1056, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.387, words per second = 1314, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.341, words per second = 1571, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.245, words per second = 1826, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.992, words per second = 2079, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.369, words per second = 2332, lr = 0.407, since beginning = 6 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.246, words per second = 2584, lr = 0.407, since beginning = 6 mins, \n",
            "Epoch 24: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 24: Train Loss: 4.236\n",
            "Epoch 24: Train Perplexity: 69.414\n",
            "Epoch 24: Validation Perplexity: 108.518\n",
            "Epoch 24: Test Perplexity: 104.542\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.941, words per second = 1, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.129, words per second = 256, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.239, words per second = 509, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.135, words per second = 760, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.038, words per second = 1010, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.331, words per second = 1258, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.396, words per second = 1505, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.360, words per second = 1751, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 4.013, words per second = 1995, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.305, words per second = 2239, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.232, words per second = 2480, lr = 0.239, since beginning = 6 mins, \n",
            "Epoch 25: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 25: Train Loss: 4.212\n",
            "Epoch 25: Train Perplexity: 67.889\n",
            "Epoch 25: Validation Perplexity: 107.355\n",
            "Epoch 25: Test Perplexity: 103.371\n",
            "Saw better model at Epoch 25\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.917, words per second = 1, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.108, words per second = 246, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.290, words per second = 489, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.129, words per second = 730, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.050, words per second = 970, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.253, words per second = 1209, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.307, words per second = 1447, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.221, words per second = 1683, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.950, words per second = 1918, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.329, words per second = 2152, lr = 0.239, since beginning = 6 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.238, words per second = 2385, lr = 0.239, since beginning = 6 mins, \n",
            "Epoch 26: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 26: Train Loss: 4.205\n",
            "Epoch 26: Train Perplexity: 67.465\n",
            "Epoch 26: Validation Perplexity: 107.205\n",
            "Epoch 26: Test Perplexity: 103.224\n",
            "Saw better model at Epoch 26\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.959, words per second = 1, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.026, words per second = 236, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.271, words per second = 470, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.181, words per second = 702, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.036, words per second = 933, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.312, words per second = 1164, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.338, words per second = 1393, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.252, words per second = 1620, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.969, words per second = 1847, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.287, words per second = 2072, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.292, words per second = 2297, lr = 0.239, since beginning = 7 mins, \n",
            "Epoch 27: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 27: Train Loss: 4.197\n",
            "Epoch 27: Train Perplexity: 67.086\n",
            "Epoch 27: Validation Perplexity: 107.200\n",
            "Epoch 27: Test Perplexity: 103.293\n",
            "Saw better model at Epoch 27\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.935, words per second = 1, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.057, words per second = 227, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.184, words per second = 453, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.131, words per second = 677, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.061, words per second = 900, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.329, words per second = 1122, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.315, words per second = 1342, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.287, words per second = 1562, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.941, words per second = 1781, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.186, words per second = 1998, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.241, words per second = 2215, lr = 0.239, since beginning = 7 mins, \n",
            "Epoch 28: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 28: Train Loss: 4.192\n",
            "Epoch 28: Train Perplexity: 66.513\n",
            "Epoch 28: Validation Perplexity: 107.013\n",
            "Epoch 28: Test Perplexity: 102.921\n",
            "Saw better model at Epoch 28\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.852, words per second = 1, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 3.995, words per second = 219, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.234, words per second = 437, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.143, words per second = 653, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.073, words per second = 868, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.313, words per second = 1082, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.318, words per second = 1295, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.174, words per second = 1508, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.902, words per second = 1719, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.308, words per second = 1929, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.191, words per second = 2137, lr = 0.239, since beginning = 7 mins, \n",
            "Epoch 29: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 29: Train Loss: 4.187\n",
            "Epoch 29: Train Perplexity: 66.241\n",
            "Epoch 29: Validation Perplexity: 107.144\n",
            "Epoch 29: Test Perplexity: 103.098\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.889, words per second = 1, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.139, words per second = 212, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.179, words per second = 422, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.088, words per second = 631, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.005, words per second = 839, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.317, words per second = 1046, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.301, words per second = 1252, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.207, words per second = 1457, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.974, words per second = 1661, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.242, words per second = 1863, lr = 0.239, since beginning = 7 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.157, words per second = 2065, lr = 0.239, since beginning = 7 mins, \n",
            "Epoch 30: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 30: Train Loss: 4.182\n",
            "Epoch 30: Train Perplexity: 65.869\n",
            "Epoch 30: Validation Perplexity: 106.924\n",
            "Epoch 30: Test Perplexity: 102.946\n",
            "Saw better model at Epoch 30\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.847, words per second = 1, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.021, words per second = 205, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.206, words per second = 408, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.144, words per second = 610, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.042, words per second = 811, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.279, words per second = 1012, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.357, words per second = 1211, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.243, words per second = 1409, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.830, words per second = 1606, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.256, words per second = 1802, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.183, words per second = 1998, lr = 0.141, since beginning = 8 mins, \n",
            "Epoch 31: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 31: Train Loss: 4.169\n",
            "Epoch 31: Train Perplexity: 65.049\n",
            "Epoch 31: Validation Perplexity: 106.391\n",
            "Epoch 31: Test Perplexity: 102.305\n",
            "Saw better model at Epoch 31\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.839, words per second = 1, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.030, words per second = 198, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.250, words per second = 395, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.162, words per second = 591, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.004, words per second = 786, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.350, words per second = 979, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.280, words per second = 1172, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.168, words per second = 1364, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.844, words per second = 1555, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.287, words per second = 1746, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.217, words per second = 1935, lr = 0.141, since beginning = 8 mins, \n",
            "Epoch 32: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 32: Train Loss: 4.164\n",
            "Epoch 32: Train Perplexity: 64.754\n",
            "Epoch 32: Validation Perplexity: 106.266\n",
            "Epoch 32: Test Perplexity: 102.250\n",
            "Saw better model at Epoch 32\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.840, words per second = 1, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 3.989, words per second = 192, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.232, words per second = 383, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.148, words per second = 573, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.051, words per second = 761, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.275, words per second = 949, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.384, words per second = 1136, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.275, words per second = 1322, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.999, words per second = 1508, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.288, words per second = 1693, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.271, words per second = 1877, lr = 0.141, since beginning = 8 mins, \n",
            "Epoch 33: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 33: Train Loss: 4.160\n",
            "Epoch 33: Train Perplexity: 64.660\n",
            "Epoch 33: Validation Perplexity: 106.326\n",
            "Epoch 33: Test Perplexity: 102.198\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.844, words per second = 1, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.062, words per second = 186, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.210, words per second = 371, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.102, words per second = 555, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.040, words per second = 738, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.297, words per second = 921, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.336, words per second = 1102, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.228, words per second = 1283, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.915, words per second = 1464, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.217, words per second = 1643, lr = 0.141, since beginning = 8 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.159, words per second = 1822, lr = 0.141, since beginning = 8 mins, \n",
            "Epoch 34: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 34: Train Loss: 4.157\n",
            "Epoch 34: Train Perplexity: 64.212\n",
            "Epoch 34: Validation Perplexity: 106.306\n",
            "Epoch 34: Test Perplexity: 102.117\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.833, words per second = 1, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.022, words per second = 181, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.230, words per second = 360, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.121, words per second = 539, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.104, words per second = 717, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.285, words per second = 894, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.375, words per second = 1071, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.195, words per second = 1247, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.864, words per second = 1422, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.269, words per second = 1596, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.210, words per second = 1770, lr = 0.141, since beginning = 9 mins, \n",
            "Epoch 35: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 35: Train Loss: 4.153\n",
            "Epoch 35: Train Perplexity: 64.074\n",
            "Epoch 35: Validation Perplexity: 106.338\n",
            "Epoch 35: Test Perplexity: 102.102\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.873, words per second = 1, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.018, words per second = 176, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.236, words per second = 350, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.161, words per second = 524, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 4.047, words per second = 697, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.331, words per second = 869, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.331, words per second = 1041, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.190, words per second = 1212, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.925, words per second = 1382, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.207, words per second = 1552, lr = 0.141, since beginning = 9 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.187, words per second = 1721, lr = 0.141, since beginning = 9 mins, \n",
            "Epoch 36: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 36: Train Loss: 4.151\n",
            "Epoch 36: Train Perplexity: 63.732\n",
            "Epoch 36: Validation Perplexity: 106.202\n",
            "Epoch 36: Test Perplexity: 102.052\n",
            "Saw better model at Epoch 36\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.812, words per second = 1, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.004, words per second = 171, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.264, words per second = 340, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.112, words per second = 509, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 3.995, words per second = 678, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.254, words per second = 845, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.328, words per second = 1012, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.238, words per second = 1179, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.885, words per second = 1344, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.297, words per second = 1510, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.200, words per second = 1674, lr = 0.083, since beginning = 9 mins, \n",
            "Epoch 37: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 37: Train Loss: 4.144\n",
            "Epoch 37: Train Perplexity: 63.282\n",
            "Epoch 37: Validation Perplexity: 105.918\n",
            "Epoch 37: Test Perplexity: 101.646\n",
            "Saw better model at Epoch 37\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.826, words per second = 1, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.091, words per second = 166, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.199, words per second = 331, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.106, words per second = 496, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 3.998, words per second = 660, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.244, words per second = 823, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.242, words per second = 985, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.141, words per second = 1147, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.899, words per second = 1309, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.268, words per second = 1469, lr = 0.083, since beginning = 9 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.156, words per second = 1629, lr = 0.083, since beginning = 10 mins, \n",
            "Epoch 38: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 38: Train Loss: 4.142\n",
            "Epoch 38: Train Perplexity: 63.137\n",
            "Epoch 38: Validation Perplexity: 105.819\n",
            "Epoch 38: Test Perplexity: 101.536\n",
            "Saw better model at Epoch 38\n",
            "batch no = 0 / 2323, avg train loss per word this batch = 4.835, words per second = 1, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 232 / 2323, avg train loss per word this batch = 4.016, words per second = 162, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 464 / 2323, avg train loss per word this batch = 4.164, words per second = 323, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 696 / 2323, avg train loss per word this batch = 4.127, words per second = 483, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 928 / 2323, avg train loss per word this batch = 3.987, words per second = 642, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 1160 / 2323, avg train loss per word this batch = 4.229, words per second = 801, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 1392 / 2323, avg train loss per word this batch = 4.300, words per second = 960, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 1624 / 2323, avg train loss per word this batch = 4.257, words per second = 1117, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 1856 / 2323, avg train loss per word this batch = 3.882, words per second = 1274, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 2088 / 2323, avg train loss per word this batch = 4.236, words per second = 1431, lr = 0.083, since beginning = 10 mins, \n",
            "batch no = 2320 / 2323, avg train loss per word this batch = 4.233, words per second = 1587, lr = 0.083, since beginning = 10 mins, \n",
            "Epoch 39: Start Learning Rate: 2.0, Dropout: 0.25\n",
            "Epoch 39: Train Loss: 4.139\n",
            "Epoch 39: Train Perplexity: 63.076\n",
            "Epoch 39: Validation Perplexity: 105.867\n",
            "Epoch 39: Test Perplexity: 101.572\n",
            "Test Set Perplexity: 101.536 Model: GRU Dropout: 0.25 Hidden_size: 200\n",
            "Training complete. Best model saved.\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 20\n",
        "seq_length = 20\n",
        "hidden_size = 200\n",
        "layer_num = 2\n",
        "max_grad_norm = 5\n",
        "\n",
        "total_epochs = 13\n",
        "dropout = 0.0\n",
        "\n",
        "# only used for paper implentation of decay after threshold\n",
        "epoch_threshold = 7\n",
        "lr_decay = 0.5\n",
        "\n",
        "# Initialize datasets\n",
        "trn, vld, tst, vocab_size = data_init()\n",
        "\n",
        "trn = minibatch(trn, batch_size, seq_length)\n",
        "vld = minibatch(vld, batch_size, seq_length)\n",
        "tst = minibatch(tst, batch_size, seq_length)\n",
        "\n",
        "def run_experiments_no_dropout():\n",
        "\n",
        "    dropout = 0.0\n",
        "    total_epochs = 15\n",
        "    learning_rate = 2.0\n",
        "    step_size = 5\n",
        "    gamma = 0.5\n",
        "\n",
        "    for rnn_type in ['LSTM', 'GRU']:\n",
        "        model = Model(vocab_size, hidden_size, layer_num, dropout, rnn_type=rnn_type).to(device)\n",
        "        train((trn, vld, tst), model, total_epochs, learning_rate, max_grad_norm, epoch_threshold, lr_decay, step_size, gamma, dropout=False)\n",
        "\n",
        "def run_experiments_with_dropout():\n",
        "\n",
        "    total_epochs = 25\n",
        "    step_size = 6\n",
        "    gamma = 1.0/1.65\n",
        "\n",
        "    for dropout in [0.25]:\n",
        "      for rnn_type in ['LSTM', 'GRU']:\n",
        "          model = Model(vocab_size, hidden_size, layer_num, dropout, rnn_type=rnn_type).to(device)\n",
        "          learning_rate = 4.0 if rnn_type=='LSTM' else 2.0\n",
        "          train((trn, vld, tst), model, total_epochs, learning_rate, max_grad_norm, epoch_threshold, lr_decay, step_size, gamma, dropout=True)\n",
        "\n",
        "# Call the function to run experiments\n",
        "run_experiments_no_dropout()\n",
        "run_experiments_with_dropout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table"
      ],
      "metadata": {
        "id": "EOQBsbysN5jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Perplexity values are from WandB runs history\n",
        "data = {\n",
        "    'Model': ['LSTM No Dropout', 'GRU No Dropout', 'LSTM 25% Dropout', 'GRU 25% Dropout'],\n",
        "    'Training Perplexity': [72.49, 67.35, 67.97, 59.98],\n",
        "    'Validation Perplexity': [123.75, 124.40, 102.66, 104.49],\n",
        "    'Test Perplexity': [119.82, 119.98, 99.08, 100.88]\n",
        "}\n",
        "\n",
        "# Display table\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(values=list(df.columns),\n",
        "                fill_color='coral',\n",
        "                align='left'),\n",
        "    cells=dict(values=[df[col] for col in df.columns],\n",
        "               fill_color='lavender',\n",
        "               align='left'))\n",
        "])\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Perplexities of Various Models\",\n",
        "    height=500,\n",
        "    width=750\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "7ks-fBzgt80_",
        "outputId": "616bc8c0-e3b3-4600-9e52-c31e0fe83c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4d6d6a62-8eff-45a0-828d-5e802f707149\" class=\"plotly-graph-div\" style=\"height:500px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4d6d6a62-8eff-45a0-828d-5e802f707149\")) {                    Plotly.newPlot(                        \"4d6d6a62-8eff-45a0-828d-5e802f707149\",                        [{\"cells\":{\"align\":\"left\",\"fill\":{\"color\":\"lavender\"},\"values\":[[\"LSTM No Dropout\",\"GRU No Dropout\",\"LSTM 25% Dropout\",\"GRU 25% Dropout\"],[72.49,67.35,67.97,59.98],[123.75,124.4,102.66,104.49],[119.82,119.98,99.08,100.88]]},\"header\":{\"align\":\"left\",\"fill\":{\"color\":\"coral\"},\"values\":[\"Model\",\"Training Perplexity\",\"Validation Perplexity\",\"Test Perplexity\"]},\"type\":\"table\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Perplexities of Various Models\"},\"height\":500,\"width\":750},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4d6d6a62-8eff-45a0-828d-5e802f707149');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5f43e7bd7914a559f06c069630e8e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f25d88e25c341bbae6b550e403a4aca",
              "IPY_MODEL_5638374ee6694af8b9e07ba2eb04613c"
            ],
            "layout": "IPY_MODEL_9a35e839a51a4dbc92c5623dd3b372a2"
          }
        },
        "0f25d88e25c341bbae6b550e403a4aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9148e9f659b44c65b2ee98bbc94bcc64",
            "placeholder": "​",
            "style": "IPY_MODEL_64afe86788344687a64dbed17c2bcc7b",
            "value": "0.036 MB of 0.036 MB uploaded\r"
          }
        },
        "5638374ee6694af8b9e07ba2eb04613c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cf89f6e736443d593c25faa73389ee2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49e4ba3f8770464c84735cbbdfddc90b",
            "value": 1
          }
        },
        "9a35e839a51a4dbc92c5623dd3b372a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9148e9f659b44c65b2ee98bbc94bcc64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64afe86788344687a64dbed17c2bcc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cf89f6e736443d593c25faa73389ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49e4ba3f8770464c84735cbbdfddc90b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}